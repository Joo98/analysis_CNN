{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wngud\\anaconda3\\envs\\ODE\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "train_size = int(0.9 * len(trainset))\n",
    "valid_size = int(0.1 * len(trainset))\n",
    "_,validset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size = 100, shuffle = False, num_workers = 2\n",
    ")\n",
    "\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck_with_Tanh(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck_with_Tanh, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.bn1(self.conv1(x)))\n",
    "        out = torch.tanh(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_with_Tanh(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet_with_Tanh, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck_with_Relu(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck_with_Relu, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_with_Relu(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet_with_Relu, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck_without_residual_learning(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck_without_residual_learning, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        #out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_without_residual_learning(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet_without_residual_learning, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50_with_Tanh():\n",
    "    return ResNet_with_Tanh(Bottleneck_with_Tanh, [3, 4, 6, 3])\n",
    "def ResNet50_with_Relu():\n",
    "    return ResNet_with_Relu(Bottleneck_with_Relu, [3, 4, 6, 3])\n",
    "def ResNet50_without_residual_learning():\n",
    "    return ResNet_without_residual_learning(Bottleneck_without_residual_learning, [3, 4, 6, 3])\n",
    "def ResNet152_with_Tanh():\n",
    "    return ResNet_with_Tanh(Bottleneck_with_Tanh, [3, 8, 36, 3])\n",
    "def ResNet152_with_Relu():\n",
    "    return ResNet_with_Relu(Bottleneck_with_Relu, [3, 8, 36, 3])\n",
    "def ResNet152_without_residual_learning():\n",
    "    return ResNet_without_residual_learning(Bottleneck_without_residual_learning, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.1+cu116, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}, Device: {DEVICE}\")\n",
    "\n",
    "model = ResNet152_with_Tanh().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, bar):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 30 == 0:\n",
    "            bar.set_postfix(\n",
    "                Train_Loss=f\"{loss.item():0.3f}\",\n",
    "            )\n",
    "    return loss_sum / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_as_list(model,layer):\n",
    "    ret = {}\n",
    "    if layer == 1:\n",
    "        for name,p in model.layer1._modules['0'].conv2.named_parameters():\n",
    "            ret[name] = p.cpu().detach()\n",
    "    elif layer == 4:\n",
    "        for name,p in model.layer4._modules['0'].conv2.named_parameters():\n",
    "            ret[name] = p.cpu().detach()\n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation(Relu vs Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 100/100 [6:40:54<00:00, 240.55s/it, Train_Loss=0.543] \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "train_loss_save = []\n",
    "test_loss_save = []\n",
    "train_acc_save = []\n",
    "test_acc_save = []\n",
    "bar = tqdm(range(EPOCHS), total=EPOCHS, desc='Train ')\n",
    "writer = SummaryWriter()\n",
    "Conv_1 = parameters_as_list(model,1)\n",
    "Conv_5 = parameters_as_list(model,4)\n",
    "for Epoch in bar:\n",
    "    train(model, train_loader, optimizer, bar)\n",
    "    \n",
    "    new_Conv_1 = parameters_as_list(model,1)\n",
    "    new_Conv_5 = parameters_as_list(model,4)\n",
    "    if Epoch % 10 == 0:\n",
    "        layer_1_weight_gap = mse_loss(new_Conv_1['weight'],Conv_1['weight'])\n",
    "        layer_5_weight_gap = mse_loss(new_Conv_5['weight'],Conv_5['weight'])\n",
    "        writer.add_scalar(\"parameter_gap/layer_1\", layer_1_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"parameter_gap/layer_5\", layer_5_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"score\", layer_1_weight_gap / layer_5_weight_gap, Epoch)\n",
    "    Conv_1 = new_Conv_1\n",
    "    Conv_5 = new_Conv_5    \n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.1+cu116, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}, Device: {DEVICE}\")\n",
    "\n",
    "model = ResNet152_with_Relu().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 100/100 [6:44:48<00:00, 242.89s/it, Train_Loss=0.067] \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "train_loss_save = []\n",
    "test_loss_save = []\n",
    "train_acc_save = []\n",
    "test_acc_save = []\n",
    "bar = tqdm(range(EPOCHS), total=EPOCHS, desc='Train ')\n",
    "writer = SummaryWriter()\n",
    "Conv_1 = parameters_as_list(model,1)\n",
    "Conv_5 = parameters_as_list(model,4)\n",
    "for Epoch in bar:\n",
    "    train(model, train_loader, optimizer, bar)\n",
    "    \n",
    "    new_Conv_1 = parameters_as_list(model,1)\n",
    "    new_Conv_5 = parameters_as_list(model,4)\n",
    "    if Epoch % 10 == 0:\n",
    "        layer_1_weight_gap = mse_loss(new_Conv_1['weight'],Conv_1['weight'])\n",
    "        layer_5_weight_gap = mse_loss(new_Conv_5['weight'],Conv_5['weight'])\n",
    "        writer.add_scalar(\"parameter_gap/layer_1\", layer_1_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"parameter_gap/layer_5\", layer_5_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"score\", layer_1_weight_gap / layer_5_weight_gap, Epoch)\n",
    "    Conv_1 = new_Conv_1\n",
    "    Conv_5 = new_Conv_5    \n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.1+cu116, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}, Device: {DEVICE}\")\n",
    "\n",
    "model = ResNet50_without_residual_learning().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train : 100%|██████████| 100/100 [2:19:03<00:00, 83.43s/it, Train_Loss=0.038] \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "train_loss_save = []\n",
    "test_loss_save = []\n",
    "train_acc_save = []\n",
    "test_acc_save = []\n",
    "bar = tqdm(range(EPOCHS), total=EPOCHS, desc='Train ')\n",
    "writer = SummaryWriter(comment=\"50_without_Relu\")\n",
    "Conv_1 = parameters_as_list(model,1)\n",
    "Conv_5 = parameters_as_list(model,4)\n",
    "for Epoch in bar:\n",
    "    train(model, train_loader, optimizer, bar)\n",
    "    \n",
    "    new_Conv_1 = parameters_as_list(model,1)\n",
    "    new_Conv_5 = parameters_as_list(model,4)\n",
    "    if Epoch % 10 == 0:\n",
    "        layer_1_weight_gap = mse_loss(new_Conv_1['weight'],Conv_1['weight'])\n",
    "        layer_5_weight_gap = mse_loss(new_Conv_5['weight'],Conv_5['weight'])\n",
    "        writer.add_scalar(\"parameter_gap/layer_1\", layer_1_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"parameter_gap/layer_5\", layer_5_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"score\", layer_1_weight_gap / layer_5_weight_gap, Epoch)\n",
    "    Conv_1 = new_Conv_1\n",
    "    Conv_5 = new_Conv_5    \n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.1+cu116, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}, Device: {DEVICE}\")\n",
    "\n",
    "model = ResNet152_without_residual_learning().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :   0%|          | 0/100 [00:10<?, ?it/s]\n",
      "Train :  92%|█████████▏| 92/100 [5:28:15<28:32, 214.08s/it, Train_Loss=1.665]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wngud\\OneDrive\\문서\\CVproject\\Analysis_Activation.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m Conv_5 \u001b[39m=\u001b[39m parameters_as_list(model,\u001b[39m4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m Epoch \u001b[39min\u001b[39;00m bar:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, bar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     new_Conv_1 \u001b[39m=\u001b[39m parameters_as_list(model,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     new_Conv_5 \u001b[39m=\u001b[39m parameters_as_list(model,\u001b[39m4\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\wngud\\OneDrive\\문서\\CVproject\\Analysis_Activation.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, bar)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output \u001b[39m=\u001b[39m model(image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wngud/OneDrive/%EB%AC%B8%EC%84%9C/CVproject/Analysis_Activation.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "train_loss_save = []\n",
    "test_loss_save = []\n",
    "train_acc_save = []\n",
    "test_acc_save = []\n",
    "bar = tqdm(range(EPOCHS), total=EPOCHS, desc='Train ')\n",
    "writer = SummaryWriter(comment=\"152_without_Relu\")\n",
    "Conv_1 = parameters_as_list(model,1)\n",
    "Conv_5 = parameters_as_list(model,4)\n",
    "for Epoch in bar:\n",
    "    train_loss = train(model, train_loader, optimizer, bar)\n",
    "    \n",
    "    new_Conv_1 = parameters_as_list(model,1)\n",
    "    new_Conv_5 = parameters_as_list(model,4)\n",
    "    writer.add_scalar(\"train/loss\", train_loss, Epoch)\n",
    "    if Epoch % 10 == 0:\n",
    "        layer_1_weight_gap = mse_loss(new_Conv_1['weight'],Conv_1['weight'])\n",
    "        layer_5_weight_gap = mse_loss(new_Conv_5['weight'],Conv_5['weight'])\n",
    "        \n",
    "        writer.add_scalar(\"parameter_gap/layer_1\", layer_1_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"parameter_gap/layer_5\", layer_5_weight_gap, Epoch)\n",
    "        writer.add_scalar(\"score\", layer_1_weight_gap / layer_5_weight_gap, Epoch)\n",
    "    Conv_1 = new_Conv_1\n",
    "    Conv_5 = new_Conv_5    \n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
